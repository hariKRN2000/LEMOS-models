{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb6c5db",
   "metadata": {},
   "source": [
    "### This file runs the random trajectory analysis for plotting performance maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1189c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lmfit\n",
    "\n",
    "# run_constant is the simulator that runs the open loop experiment\n",
    "from model_equations_and_simulators.run_constant import run_constant\n",
    "\n",
    "# run_p_control is the simulator that runs the closed loop experiment with P control\n",
    "from model_equations_and_simulators.run_p_control import run_p_control\n",
    "\n",
    "# run_pi_control is the simulator that runs the closed loop experiment with PI control\n",
    "from model_equations_and_simulators.run_pi_control import run_pi_control\n",
    "\n",
    "# run_pi_control is the simulator that runs the closed loop experiment with PI control\n",
    "from model_equations_and_simulators.run_pid_control import run_pid_control\n",
    "\n",
    "# signal_analysis contains the functions to calculate the performance metrics\n",
    "from signal_analysis import analyze_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1b7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets for P control\n",
    "data1 = pd.read_csv('experiment_data/P-FL_OD_run_data_042325.csv')\n",
    "data2 = pd.read_csv('experiment_data/P-FL_OD_run_data_040625.csv')\n",
    "\n",
    "# Extract relevant wells\n",
    "P1_data1 = data1[['SP1_1', 'SP1_2', 'SP1_3', 'SP1_4', 'SP1_5', 'SP1_6']].to_numpy()\n",
    "P2_data1 = data1[['SP2_1', 'SP2_2', 'SP2_3', 'SP2_4', 'SP2_5', 'SP2_6']].to_numpy()\n",
    "P1_data2 = data2[['SP1_1', 'SP1_2', 'SP1_3']].to_numpy()\n",
    "P2_data2 = data2[['SP2_1', 'SP2_2', 'SP2_3']].to_numpy()\n",
    "\n",
    "green_data1 = data1[['G1']].to_numpy()\n",
    "red_data1 = data1[['R1']].to_numpy()\n",
    "\n",
    "green_data2 = data2[['G1', 'G2', 'G3']].to_numpy()\n",
    "red_data2 = data2[['R1', 'R2', 'R3']].to_numpy()\n",
    "\n",
    "# Time axis\n",
    "interval = 10\n",
    "min_len = min(P1_data1.shape[0], P1_data2.shape[0])\n",
    "time_exp_P = np.arange(interval, (min_len + 1) * interval, interval)\n",
    "\n",
    "# Truncate to matching length\n",
    "green_data1 = green_data1[:min_len, :]\n",
    "green_data2 = green_data2[:min_len, :]\n",
    "red_data1 = red_data1[:min_len, :]\n",
    "red_data2 = red_data2[:min_len, :]\n",
    "P1_data1 = P1_data1[:min_len, :]\n",
    "P1_data2 = P1_data2[:min_len, :]\n",
    "P2_data1 = P2_data1[:min_len, :]\n",
    "P2_data2 = P2_data2[:min_len, :]\n",
    "\n",
    "# Combine datasets\n",
    "green_combined = np.hstack([green_data1, green_data2])\n",
    "red_combined = np.hstack([red_data1, red_data2])\n",
    "P1_combined = np.hstack([P1_data1, P1_data2])\n",
    "P2_combined = np.hstack([P2_data1, P2_data2])\n",
    "\n",
    "# Compute mean profiles\n",
    "green_mean_P = np.mean(green_combined, axis=1)\n",
    "red_mean_P = np.mean(red_combined, axis=1)\n",
    "P1_mean = np.mean(P1_combined, axis=1)\n",
    "P2_mean = np.mean(P2_combined, axis=1)\n",
    "\n",
    "# Use green max from experimental data\n",
    "green_max_expt_P = np.max(green_mean_P)\n",
    "\n",
    "# Setpoint values\n",
    "st_pt_1 = 11500\n",
    "st_pt_2 = 18500\n",
    "\n",
    "# === Compute std for plotting shaded regions ===\n",
    "\n",
    "# Standard deviations\n",
    "P1_std = np.std(P1_combined, axis=1)\n",
    "P2_std = np.std(P2_combined, axis=1)\n",
    "green_std = np.std(green_combined, axis=1)\n",
    "red_std = np.std(red_combined, axis=1)\n",
    "\n",
    "# Normalize std\n",
    "P1_std_norm = P1_std / green_max_expt_P\n",
    "P2_std_norm = P2_std / green_max_expt_P\n",
    "green_std_norm = green_std / green_max_expt_P\n",
    "red_std_norm = red_std / green_max_expt_P\n",
    "\n",
    "# Also normalize mean values for plotting\n",
    "P1_mean_norm = P1_mean / green_max_expt_P\n",
    "P2_mean_norm = P2_mean / green_max_expt_P\n",
    "green_mean_norm = green_mean_P / green_max_expt_P\n",
    "red_mean_norm = red_mean_P / green_max_expt_P\n",
    "\n",
    "\n",
    "# Color dictionary\n",
    "color_dict = {\n",
    "    'set_point_1': '#6759d4',\n",
    "    'set_point_2': '#ebab4b',\n",
    "    'green': 'green',\n",
    "    'red': 'red'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99424bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for PI control \n",
    "\n",
    "data1_PI = pd.read_csv('experiment_data/PI-FL_OD_run_data_042525.csv')\n",
    "data2_PI = pd.read_csv('experiment_data/PI-FL_OD_run_data_041025.csv')\n",
    "\n",
    "PI1_data1 = data1_PI[['SP1_1',  'SP1_2',  'SP1_3']].to_numpy()\n",
    "PI2_data1 = data1_PI[['SP2_1',  'SP2_2',  'SP2_3']].to_numpy()\n",
    "\n",
    "PI1_data2 = data2_PI[['SP1_1',  'SP1_2',  'SP1_3']].to_numpy()\n",
    "PI2_data2 = data2_PI[['SP2_1',  'SP2_2',  'SP2_3']].to_numpy()\n",
    "\n",
    "# Extract Green/Red constants (with replicates for 040825)\n",
    "red_data1_PI = data1_PI[['R1',  'R2',  'R3']].to_numpy()\n",
    "green_data1_PI = data1_PI[['G1',  'G2', 'G3']].to_numpy()\n",
    "\n",
    "# For 0401025, Green and Red have only 1 well each\n",
    "green_data2_PI = data2_PI[['G1']].to_numpy()\n",
    "red_data2_PI = data2_PI[['R1']].to_numpy()\n",
    "\n",
    "# Truncate all data to shared minimum length\n",
    "min_len_PI = min(PI1_data1.shape[0], PI1_data2.shape[0])\n",
    "time_exp_PI = np.arange(interval, (min_len_PI + 1) * interval, interval)\n",
    "P1_data1_PI = PI1_data1[:min_len_PI]; P2_data1_PI = PI2_data1[:min_len_PI]\n",
    "P1_data2_PI = PI1_data2[:min_len_PI]; P2_data2_PI = PI2_data2[:min_len_PI]\n",
    "green_data1_PI = green_data1_PI[:min_len_PI]; green_data2_PI = green_data2_PI[:min_len_PI]\n",
    "red_data1_PI = red_data1_PI[:min_len_PI]; red_data2_PI = red_data2_PI[:min_len_PI]\n",
    "\n",
    "# Merge all experimental replicates\n",
    "P1_combined_PI = np.hstack((P1_data1_PI, P1_data2_PI))\n",
    "P2_combined_PI = np.hstack((P2_data1_PI, P2_data2_PI))\n",
    "green_combined_PI = np.hstack((green_data1_PI, green_data2_PI))\n",
    "red_combined_PI = np.hstack((red_data1_PI, red_data2_PI))\n",
    "\n",
    "# Compute means and standard deviations\n",
    "P1_mean_PI = np.mean(P1_combined_PI, axis=1)\n",
    "P1_std_PI = np.std(P1_combined_PI, axis=1)\n",
    "P2_mean_PI = np.mean(P2_combined_PI, axis=1)\n",
    "P2_std_PI = np.std(P2_combined_PI, axis=1)\n",
    "green_mean_PI = np.mean(green_combined_PI, axis=1)\n",
    "green_std_PI = np.std(green_combined_PI, axis=1)\n",
    "red_mean_PI = np.mean(red_combined_PI, axis=1)\n",
    "red_std_PI = np.std(red_combined_PI, axis=1)\n",
    "\n",
    "# Use green max from experimental data\n",
    "green_max_expt_PI = np.max(green_mean_PI)\n",
    "\n",
    "# Normalize std\n",
    "P1_std_norm_PI = P1_std_PI / green_max_expt_P\n",
    "P2_std_norm_PI = P2_std_PI / green_max_expt_P\n",
    "green_std_norm_PI = green_std_PI / green_max_expt_P\n",
    "red_std_norm_PI = red_std_PI / green_max_expt_P\n",
    "\n",
    "# Also normalize mean values for plotting\n",
    "P1_mean_norm_PI = P1_mean_PI / green_max_expt_P\n",
    "P2_mean_norm_PI = P2_mean_PI / green_max_expt_P\n",
    "green_mean_norm_PI = green_mean_PI / green_max_expt_P\n",
    "red_mean_norm_PI = red_mean_PI / green_max_expt_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff3f64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for PID control \n",
    "\n",
    "data1_PID = pd.read_csv('experiment_data/PID-FL_OD_run_data_050725.csv')\n",
    "data2_PID = pd.read_csv('experiment_data/PID-FL_OD_run_data_050925.csv')\n",
    "\n",
    "PID1_data1 = data1_PID[['SP1_1',  'SP1_2',  'SP1_3']].to_numpy()\n",
    "PID2_data1 = data1_PID[['SP2_1',  'SP2_2',  'SP2_3']].to_numpy()\n",
    "\n",
    "PID1_data2 = data2_PID[['SP1_1',  'SP1_2',  'SP1_3']].to_numpy()\n",
    "PID2_data2 = data2_PID[['SP2_1',  'SP2_2',  'SP2_3']].to_numpy()\n",
    "\n",
    "red_data1_PID = data1_PID[['R1',  'R2',  'R3']].to_numpy()\n",
    "green_data1_PID = data1_PID[['G1',  'G2', 'G3']].to_numpy()\n",
    "\n",
    "red_data2_PID = data2_PID[['R1',  'R2',  'R3']].to_numpy()\n",
    "green_data2_PID = data2_PID[['G1',  'G2', 'G3']].to_numpy()\n",
    "\n",
    "# Truncate all data to shared minimum length\n",
    "min_len_PID = min(PID1_data1.shape[0], PID1_data2.shape[0])\n",
    "time_exp_PID = np.arange(interval, (min_len_PID + 1) * interval, interval)\n",
    "P1_data1_PID = PID1_data1[:min_len_PID]; P2_data1_PID = PID2_data1[:min_len_PID]\n",
    "P1_data2_PID = PID1_data2[:min_len_PID]; P2_data2_PID = PID2_data2[:min_len_PID]\n",
    "green_data1_PID = green_data1_PID[:min_len_PID]; green_data2_PID = green_data2_PID[:min_len_PID]\n",
    "red_data1_PID = red_data1_PID[:min_len_PID]; red_data2_PID = red_data2_PID[:min_len_PID]\n",
    "\n",
    "# Merge all experimental replicates\n",
    "P1_combined_PID = np.hstack((P1_data1_PID, P1_data2_PID))\n",
    "P2_combined_PID = np.hstack((P2_data1_PID, P2_data2_PID))\n",
    "green_combined_PID = np.hstack((green_data1_PID, green_data2_PID))\n",
    "red_combined_PID = np.hstack((red_data1_PID, red_data2_PID))\n",
    "\n",
    "# Compute means and standard deviations\n",
    "P1_mean_PID = np.mean(P1_combined_PID, axis=1)\n",
    "P1_std_PID = np.std(P1_combined_PID, axis=1)\n",
    "P2_mean_PID = np.mean(P2_combined_PID, axis=1)\n",
    "P2_std_PID = np.std(P2_combined_PID, axis=1)\n",
    "green_mean_PID = np.mean(green_combined_PID, axis=1)\n",
    "green_std_PID = np.std(green_combined_PID, axis=1)\n",
    "red_mean_PID = np.mean(red_combined_PID, axis=1)\n",
    "red_std_PID = np.std(red_combined_PID, axis=1)\n",
    "\n",
    "# Use green max from experimental data\n",
    "green_max_expt_PID = np.max(green_mean_PID)\n",
    "\n",
    "# Normalize std\n",
    "P1_std_norm_PID = P1_std_PID / green_max_expt_P\n",
    "P2_std_norm_PID = P2_std_PID / green_max_expt_P\n",
    "green_std_norm_PID = green_std_PID / green_max_expt_P\n",
    "red_std_norm_PID = red_std_PID / green_max_expt_P\n",
    "\n",
    "# Also normalize mean values for plotting\n",
    "P1_mean_norm_PID = P1_mean_PID / green_max_expt_P\n",
    "P2_mean_norm_PID = P2_mean_PID / green_max_expt_P\n",
    "green_mean_norm_PID = green_mean_PID / green_max_expt_P\n",
    "red_mean_norm_PID = red_mean_PID / green_max_expt_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d58d8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the constant green/red data\n",
    "min_len = 102\n",
    "\n",
    "# Combine based on  all three data sets\n",
    "green_combined = np.hstack([green_data1[:min_len], green_data2[:min_len], green_data1_PI[:min_len], green_data2_PI[:min_len], green_data1_PID[:min_len], green_data2_PID[:min_len]])\n",
    "red_combined = np.hstack([red_data1[:min_len], red_data2[:min_len], red_data1_PI[:min_len], red_data2_PI[:min_len], red_data1_PID[:min_len], red_data2_PID[:min_len]])\n",
    "\n",
    "green_mean_combined = np.mean(green_combined, axis=1)\n",
    "green_std_combined = np.std(green_combined, axis=1)\n",
    "green_max_combined = np.max(green_mean_combined)\n",
    "\n",
    "red_mean_combined = np.mean(red_combined, axis=1)\n",
    "red_std_combined = np.std(red_combined, axis=1)\n",
    "red_max_combined = np.max(red_mean_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e69d04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_pid_trajectories(params, x0, st_pt_1, st_pt_2, green_max_expt_P, t_final = 960):\n",
    "    # Determine green_max_model for normalization\n",
    "    _, _, sol_green = run_constant(t_final, x0, params, constant_input='green')\n",
    "    green_max_model = np.max(sol_green[:, 8])\n",
    "\n",
    "    scaling_factor = (green_max_expt_P / green_max_model) * (1/60) \n",
    "\n",
    "    # Convert setpoints to model scale\n",
    "    st_pt_1_model = st_pt_1 * (green_max_model / green_max_expt_P)\n",
    "    st_pt_2_model = st_pt_2 * (green_max_model / green_max_expt_P)\n",
    "\n",
    "    # Gains\n",
    "    p_gain = 0.064 * scaling_factor\n",
    "    pi_gain = 0.013 * scaling_factor\n",
    "    pid_gain = 0.032 * scaling_factor\n",
    "    tau_I = 1500\n",
    "    tau_D = 120\n",
    "\n",
    "    # Simulations for each controller\n",
    "    t_p1_P, pctrl_1_P, _ = run_p_control(t_final, st_pt_1_model, p_gain, x0, params)\n",
    "    t_p2_P, pctrl_2_P, _ = run_p_control(t_final, st_pt_2_model, p_gain, x0, params)\n",
    "\n",
    "    t_p1_PI, pctrl_1_PI, _, _ = run_pi_control(t_final, st_pt_1_model, pi_gain, tau_I, x0, params)\n",
    "    t_p2_PI, pctrl_2_PI, _, _ = run_pi_control(t_final, st_pt_2_model, pi_gain, tau_I, x0, params)\n",
    "\n",
    "    t_p1_PID, pctrl_1_PID, _, _ = run_pid_control(t_final, st_pt_1_model, pid_gain, tau_I, tau_D, x0, params)\n",
    "    t_p2_PID, pctrl_2_PID, _, _ = run_pid_control(t_final, st_pt_2_model, pid_gain, tau_I, tau_D, x0, params)\n",
    "\n",
    "    # Normalize\n",
    "    normalize = lambda x: np.array(x) / green_max_model\n",
    "    data = {\n",
    "        'P': {'SP1': (t_p1_P, normalize(pctrl_1_P)), 'SP2': (t_p2_P, normalize(pctrl_2_P))},\n",
    "        'PI': {'SP1': (t_p1_PI, normalize(pctrl_1_PI)), 'SP2': (t_p2_PI, normalize(pctrl_2_PI))},\n",
    "        'PID': {'SP1': (t_p1_PID, normalize(pctrl_1_PID)), 'SP2': (t_p2_PID, normalize(pctrl_2_PID))}\n",
    "    }\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "983e18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_parameters(params, n_samples = 50, sampling_fraction = 0.15):\n",
    "    \"\"\"\n",
    "    Generate random parameter sets within Â±sampling_fraction bounds.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of lmfit.Parameter objects (clones with randomized values).\n",
    "    \"\"\"\n",
    "    np.random.seed(11) # fixing for uniform sampling and replicability\n",
    "    random_sets = []\n",
    "    for _ in range(n_samples):\n",
    "        p_copy = params.copy()\n",
    "        for name, param in params.items():\n",
    "            if not param.vary:\n",
    "                continue\n",
    "            nominal = param.value\n",
    "            delta = nominal * sampling_fraction\n",
    "            new_value = np.random.uniform(nominal - delta, nominal + delta)\n",
    "            p_copy[name].value = new_value\n",
    "        random_sets.append(p_copy)\n",
    "    return random_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae74acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model parameters \n",
    "\n",
    "\n",
    "p = pd.read_csv('parameters/TCS_model_param_file.csv').to_numpy()\n",
    "p = p[:,2]\n",
    "\n",
    "params = lmfit.Parameters()\n",
    "\n",
    "params.add(name = 'k_green', value = p[0], min = 1e-4, max = 1e4, vary = 1)\n",
    "params.add(name = 'k_red', value = p[1], min = 1e0, max = 1e4, vary = 1)\n",
    "params.add(name = 'b_green', value = p[2], min = 1e-4, max = 1e4, vary = 1)\n",
    "params.add(name = 'b_red', value = p[3], min = 1e0, max = 1e4, vary = 1)\n",
    "\n",
    "params.add(name = 'k_sp_b', value = p[4], min = 1e-4, max = 1e4, vary = 1)\n",
    "params.add(name = 'k_sp_u', value = p[5], min = 1e-4, max = 1e4, vary = 1)\n",
    "\n",
    "params.add(name = 'k_rp_b', value = p[6], min = 1e-4, max = 1e4, vary = 1)\n",
    "params.add(name = 'k_rp_u', value = p[7], min = 1e-4, max = 1e4, vary = 1)\n",
    "\n",
    "params.add(name = 'beta', value = p[8], min = 1e-1, max = 500, vary = 1)\n",
    "params.add(name = 'l0', value = p[9], min = 0, max = 0.5, vary = 1)\n",
    "params.add(name = 'Kc', value = p[10], min = 1e-4, max = 1e4, vary = 1)\n",
    "params.add(name = 'd_m', value = p[11], min = 0.05, max = 0.35, vary = 1)\n",
    "params.add(name = 'k_tl', value = p[12], min = 0.05, max = 10, vary = 1)\n",
    "params.add(name = 'k_tli_b', value = p[13], min = 1e-4, max = 1e4, vary = 1)\n",
    "params.add(name = 'k_tli_u', value = p[14], min = 1e-4, max = 1e4, vary = 1)\n",
    "params.add(name = 'd_p', value = p[15], min = 1e-6, max = 1e-1, vary = 1)\n",
    "params.add(name = 'k_fold', value = p[16], min = 0.05, max = 0.3, vary = 1)\n",
    "params.add(name = 'b_fold', value = p[17], min = 0.1, max = 2, vary = 1)\n",
    "params.add(name = 'n_gamma', value = p[18], min = 0.05, max = 0.9, vary = 1)\n",
    "params.add(name = 'R_max', value = p[19], min = 1e0, max = 1e4, vary = 1)\n",
    "\n",
    "params.add(name = 'S_0', value = p[20], min = 1e-4, max = 1e4, vary = 1)\n",
    "params.add(name = 'R_0', value = p[21], min = 1e-4, max = 1e4, vary = 1)\n",
    "params.add(name = 'Sp_0', value = p[22], min = 1e-4, max = 1e4, vary = 1)\n",
    "params.add(name = 'Rp_0', value = p[23], min = 1e-4, max = 1e4, vary = 1)\n",
    "params.add(name = 'mRNA_0', value = p[24], min = 1e-4, max = 1e4, vary = 1)\n",
    "params.add(name = 'P_0', value = p[25], min = 1e-4, max = 1e4, vary = 1)\n",
    "params.add(name = 'Pm_0', value = p[26], min = 1e-4, max = 1e4, vary = 1)\n",
    "\n",
    "\n",
    "params.add(name = 'k_gr', value = p[27], vary = 0)\n",
    "params.add(name = 'C_max', value = p[28], vary = 0)\n",
    "params.add(name = 'C_0', value = p[29], vary = 0)\n",
    "\n",
    "params.add(name = 'n_tcs', value = p[30], min = 0.1, max = 5, vary = 1)\n",
    "\n",
    "\n",
    "\n",
    "# define initial conditions and solve the ODEs\n",
    "\n",
    "x0 = np.zeros(11)\n",
    "\n",
    "x0[0] = p[20] # S\n",
    "x0[1] = p[22]# Sp\n",
    "x0[2] = p[21] # R\n",
    "x0[3] = p[23] # Rp\n",
    "x0[4] = 0 # Ac\n",
    "x0[5] = p[24] # mRNA\n",
    "x0[6] = 0 # Ctic\n",
    "x0[7] = p[25] # Unfolded Protein \n",
    "x0[8] = p[26] # Folded Protein \n",
    "x0[9] = p[29] # Initial Cell count\n",
    "x0[10] = 0 # Initial intergal error\n",
    "\n",
    "# Total time of simulation\n",
    "t_final = 960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18ed70b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating for perturbation 1/50...\n",
      "Simulating for perturbation 2/50...\n",
      "Simulating for perturbation 3/50...\n",
      "Simulating for perturbation 4/50...\n",
      "Simulating for perturbation 5/50...\n",
      "Simulating for perturbation 6/50...\n",
      "Simulating for perturbation 7/50...\n",
      "Simulating for perturbation 8/50...\n",
      "Simulating for perturbation 9/50...\n",
      "Simulating for perturbation 10/50...\n",
      "Simulating for perturbation 11/50...\n",
      "Simulating for perturbation 12/50...\n",
      "Simulating for perturbation 13/50...\n",
      "Simulating for perturbation 14/50...\n",
      "Simulating for perturbation 15/50...\n",
      "Simulating for perturbation 16/50...\n",
      "Simulating for perturbation 17/50...\n",
      "Simulating for perturbation 18/50...\n",
      "Simulating for perturbation 19/50...\n",
      "Simulating for perturbation 20/50...\n",
      "Simulating for perturbation 21/50...\n",
      "Simulating for perturbation 22/50...\n",
      "Simulating for perturbation 23/50...\n",
      "Simulating for perturbation 24/50...\n",
      "Simulating for perturbation 25/50...\n",
      "Simulating for perturbation 26/50...\n",
      "Simulating for perturbation 27/50...\n",
      "Simulating for perturbation 28/50...\n",
      "Simulating for perturbation 29/50...\n",
      "Simulating for perturbation 30/50...\n",
      "Simulating for perturbation 31/50...\n",
      "Simulating for perturbation 32/50...\n",
      "Simulating for perturbation 33/50...\n",
      "Simulating for perturbation 34/50...\n",
      "Simulating for perturbation 35/50...\n",
      "Simulating for perturbation 36/50...\n",
      "Simulating for perturbation 37/50...\n",
      "Simulating for perturbation 38/50...\n",
      "Simulating for perturbation 39/50...\n",
      "Simulating for perturbation 40/50...\n",
      "Simulating for perturbation 41/50...\n",
      "Simulating for perturbation 42/50...\n",
      "Simulating for perturbation 43/50...\n",
      "Simulating for perturbation 44/50...\n",
      "Simulating for perturbation 45/50...\n",
      "Simulating for perturbation 46/50...\n",
      "Simulating for perturbation 47/50...\n",
      "Simulating for perturbation 48/50...\n",
      "Simulating for perturbation 49/50...\n",
      "Simulating for perturbation 50/50...\n",
      "Saved metrics to metrics_data/all_metrics_random_sampling.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Monte Carlo sampling + metrics\n",
    "# ----------------------------\n",
    "n_samples = 50\n",
    "interval_percentage = 0.15\n",
    "tolerance_percentage = 0.1\n",
    "\n",
    "random_param_sets = generate_random_parameters(params, n_samples=n_samples, sampling_fraction=interval_percentage)\n",
    "\n",
    "# For normalization in metrics\n",
    "green_mean_norm = green_mean_combined / green_max_combined\n",
    "red_mean_norm = red_mean_combined / green_max_combined\n",
    "max_dyn_range = float(np.max(green_mean_norm - red_mean_norm))\n",
    "tol_band = tolerance_percentage * max_dyn_range\n",
    "\n",
    "all_metrics_rows = []\n",
    "for i, param_set in enumerate(random_param_sets):\n",
    "    print(f\"Simulating for perturbation {i+1}/{n_samples}...\")\n",
    "    sim_data = simulate_pid_trajectories(param_set, x0, st_pt_1, st_pt_2, green_max_expt_P)\n",
    "\n",
    "    for sp_label, target in [('SP1', st_pt_1), ('SP2', st_pt_2)]:\n",
    "        st_norm = target / green_max_expt_P\n",
    "        for ctrl in ['P', 'PI', 'PID']:\n",
    "            t, s = sim_data[ctrl][sp_label]\n",
    "            rise_time, settling_time, offset_error = analyze_signal(\n",
    "                s, t, st_norm,\n",
    "                tolerance_band=tol_band,\n",
    "                max_dynamic_range=max_dyn_range,\n",
    "                settle_time_cap=1000\n",
    "            )\n",
    "            all_metrics_rows.append({\n",
    "                'Settling Time': settling_time,\n",
    "                'Offset Error (%)': offset_error,\n",
    "                'Control': ctrl,\n",
    "                'Perturbation': i,\n",
    "                'Set Point': sp_label\n",
    "            })\n",
    "\n",
    "all_metrics = pd.DataFrame(all_metrics_rows)\n",
    "\n",
    "# ----------------------------\n",
    "# Save for plotting\n",
    "# ----------------------------\n",
    "out_csv = 'metrics_data/all_metrics_random_sampling.csv'\n",
    "all_metrics.to_csv(out_csv, index=False)\n",
    "print(f\"Saved metrics to {out_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
